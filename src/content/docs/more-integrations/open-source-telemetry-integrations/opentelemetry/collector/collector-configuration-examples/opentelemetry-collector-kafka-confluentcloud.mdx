---
title: Collector for Confluent Cloud & Kafka monitoring
tags:
  - Integrations
  - Open source telemetry integrations
  - OpenTelemetry
  - Kafka
  - Confluent Cloud
metaDescription: You can collect Kafka metrics from Confluent using the OpenTelemetry Collector.
---

You can collect metrics about your Confluent Cloud-managed Kafka deployment with the OpenTelemetry Collector. The collector is a component of OpenTelemetry that collects, processes, and exports telemetry data to New Relic (or any observability backend).

Complete the steps below to collect Kafka metrics from Confluent using an OpenTelemetry collector running in docker.

<Steps>
    <Step>
        ## Make sure you're set up

      Before you start, you need to have the <InlinePopover type="licenseKey" /> for the account you want to report data to. You should also verify that:

      * You have a docker daemon running
      * You have [Docker Compose](https://github.com/newrelic/newrelic-opentelemetry-examples/tree/main/other-examples/collector/confluentcloud) installed
      * You can generate a [TLS authentication key](https://docs.confluent.io/platform/current/kafka/encryption.html) from your [Confluent Cloud account](https://www.confluent.io/get-started/).  

    </Step>
    <Step>
        ## Download or clone the example repos

        Download [New Relic's OpenTelemetry Examples repo](https://github.com/newrelic/newrelic-opentelemetry-examples)as this setup uses its example collector configuration. Once installed, go to the [Confluent Cloud example](https://github.com/newrelic/newrelic-opentelemetry-examples/tree/main/other-examples/collector/confluentcloud) directory. For more information, you can check the `README` there as well.

    </Step>
    <Step>
        ##  Add the pem key files and set variables

        * Add the TLS authentication keys (key.pem, cert.pem, and ca.pem files) to the `confluentcloud` directory. If you're referring to the `README`, this is step 3 in the prerequisites. 
        * Export the following variables or add them in a `.env` file, then run the `docker compose up` command.

```bash
export NEW_RELIC_API_KEY= <YOUR_API_KEY>
export NEW_RELIC_OTLP_ENDPOINT=https://otlp.nr-data.net
export CLUSTER_ID= <YOUR_CLUSTER_ID>
export CLUSTER_API_KEY=<YOUR_CLUSTER_API_KEY>
export CLUSTER_API_SECRET=<YOUR_CLUSTER_API_SECRET>
export CLUSTER_BOOTSTRAP_SERVER=<YOUR_CLUSTER_BOOTSTRAP_SERVER>

docker compose up
```

* [Kafka Client API key](https://docs.confluent.io/cloud/current/access-management/authenticate/api-keys/api-keys.html#resource-specific-api-keys)
	* `CLUSTER_API_KEY`
	* `CLUSTER_API_SECRET`
* [New Relic Ingest key](/docs/apis/intro-apis/new-relic-api-keys/#license-key)
* `CLUSTER_ID`
	* Cluster ID from Confluent cloud
	* Cluster key/secret should be specific to this cluster
* CLUSTER_BOOTSTRAP_SERVER
  * bootstrap server provided by confluent for the cluster
  * example: xxx-xxxx.us-east-2.aws.confluent.cloud:9092

    </Step>
    <Step>
        ## Create dashboard with your metric data

        Check out this New Relic [example dashboard](https://github.com/newrelic/newrelic-quickstarts/blob/main/dashboards/confluent-cloud/confluent-cloud.json) that uses metrics collected by the collector. 

    </Step>
</Steps>

### Confluent Cloud metrics [#confluent-metrics]

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Name
      </th>
      <th>
        Description
      </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        confluent_kafka_server_received_bytes
      </td>
      <td>
        The delta count of bytes of the customer's data received from the network. Each sample is the number of bytes received since the previous data sample. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_sent_bytes
      </td>
      <td>
        The delta count of bytes of the customer's data sent over the network. Each sample is the number of bytes sent since the previous data point. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_received_records
      </td>
      <td>
        The delta count of records received. Each sample is the number of records received since the previous data sample. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_sent_records
      </td>
      <td>
        The delta count of records sent. Each sample is the number of records sent since the previous data point. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_retained_bytes
      </td>
      <td>
        The current count of bytes retained by the cluster. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_active_connection_count
      </td>
      <td>
        The count of active authenticated connections.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_request_count
      </td>
      <td>
        The delta count of requests received over the network. Each sample is the number of requests received since the previous data point. The count sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_partition_count
      </td>
      <td>
        The number of partitions
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_successful_authentication_count
      </td>
      <td>
        The delta count of successful authentications. Each sample is the number of successful authentications since the previous data point. The count sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_consumer_lag_offsets
      </td>
      <td>
        The lag between a group member's committed offset and the partition's high watermark. 
      </td>
    </tr>
  </tbody>
</table>


