---
title: "Manage alert quality"
metaDescription: "How to manage the quality of your alerts with New Relic"
---

Teams suffer from alert fatigue when they experience high alert volumes and alerts that aren't aligned to business impact. High alert volumes train incident responders to assume the alerts are false and have no business impact. In turn, they may start to prioritize easy-to-resolve alerts over others and they may close unresolved incidents so they can stay within their SLA targets. This results in slower incident response and increased scope and severity when true business impacting issues occur.

Alert quality management (AQM) focuses on reducing the number of nuisance incidents so that you focus only on alerts with true business impact. This reduces alert fatigue and ensures that you and your team focus your attention on the right places at the right times.

## Why use alert quality management?

You're a good candidate for AQM if:

* You have too many alerts.
* You have alerts that stay open for long time periods.
* Your alerts are not relevant.
* Your customers discover your issues before your monitoring tools do.
* You can't see the value of your observability tool(s).

By using an alert strategy based on measuring business impact, you'll decrease response time and increase awareness of critical events. As you improve your alert signal to noise ratio, you'll reduce confusion and be able to rapidly identify and isolate the root cause of your problems.
The overall goal of alert quality management is to ensure that fewer, more valuable, incidents are created. This will result in:
Increased uptime and availability
Reduced mean time to resolution (MTTR)
Decreased alert volume
The ability to easily identify alerts that are not valuable, so you can either make them valuable or remove them.

## Using Key Performance indicators

You'll use the AQM process to collect and measure incident volume and engagement KPIs.

<CollapserGroup>
    <Collapser
        id="incident_volume"
        title="Incident volume"
    >
You should treat incidents (with or without alerts) like a queue of tasks. Just like a queue, the number of alerts should spend time near zero. Each incident should be a trigger for an investigatory or corrective action to resolve the condition. If an alert does not result in some sort of action, then you should question the value of the alert condition.
In particular, if you see specific incidents that are "always on", then you should question why. Are you in a constant state of business impact, or do you simply have a large volume of noise? The alert volume KPIs help you to answer those questions and to help you measure progress towards a healthy state of high quality alerting.

  <Collapser
    id="kpi-incident-count"
    title="Incident count KPI"
  >
Incident count is the number of incidents generated over a period of time. Typically you should compare the current and previous weeks.

**Goal:** Reduce the number of low value / nuisance incidents.

**Best practices:**
* Ensure condition settings are intended to detect real business impact.
* Ensure condition settings are detecting abnormal behavior.
* Communicate that the incident details "Acknowledge" feature helps measure meaningful and actionable alerts. See [percentage incident acknowledge KPI](#kpi-user-engagement).
* Report AQM KPIs to all stakeholders.
  </Collapser>

  <Collapser
    id="kpi-incident-duration"
    title="Accumulated incident duration KPI"
  >
Accumulated incident duration is the total sum of minutes that all incidents have accumulated over a period of time. Typically you should compare the current and previous weeks.

**Goal:** Reduce the total accumulated minutes of incidents.

**Best practices:**
* Do not manually close incidents that do not motivate some sort of investigatory action. Manual closure will skew the real duration of incident length.
* Eliminate alerts that do not result in any remediation actions from the recipients.
* Improve percent investigated and mean-time-to-investigate KPIs by communicating their importance in improving detection and response times.
* Report AQM KPIs to all stakeholders.
  </Collapser>

  <Collapser
    id="kpi-mttc"
    title="Mean time to close (MTTC) KPI"
  >
Average duration of incidents within the period of time measured.

**Goal:** Reduce MTTC

**Best practices:**
* Do not manually close incidents. Manual closure will skew the real duration of incident length.
* Improve reliability engineering skills.
* Report AQM KPIs to all stakeholders.
  </Collapser>

  <Collapser
    id="kpi-pct-under-five"
    title="Percent under 5 minutes KPI"
  >
Percentage of incidents where the duration of the incident is under five minutes. This can be an indicator of incident flapping.

**Goal:** Minimize percentage of incidents with short durations.

**Best practices:**
* Ensure that conditions are detecting legitimate deviations from expected behavior.
* Understand service level management.
* Ensure that conditions are detecting legitimate deviations that correlate to business impact or impending business impact.
  </Collapser>
</Collapser>
</CollapserGroup>

<CollapserGroup>
    <Collapser
        id="user_engagement"
        title="User engagement"
    >
You should measure the value of an incident by the amount of attention it receives. Here, we measure engagement by whether or not an incident has been acknowledged.

The amount of engagement an individual alert receives is a direct measurement of its value. More engagement implies a valuable alert. Less (or zero) engagement implies a nuisance alert that should be modified or disabled.

There is a significant difference between measuring the moment of incident awareness vs. acknowledging the moment resolution activity begins. If you are using an integration with New Relic alerts, be sure that the "acknowledge" event that is sent to New Relic is triggered when resolution activity begins, not when the incident is sent to the external incident management tool. 

For more information regarding standard incident management processes, see "[Incident management process: 5 steps to effective resolution, posted on August 31, 2020 by OnPage Corporation.](https://www.onpage.com/incident-management-process-5-steps-to-effective-resolution) in reference to [ITIL4](https://itsm.tools/its-here-itil-4-explained)"

<CollapserGroup>
  <Collapser
    id="kpi-pct-ack"
    title="Percentage acknowledged KPI"
  >
Incidents acknowledged identifies the percentage of incidents that have been engaged with by having their acknowledgement flag set to true. Typically you should compare the current and previous weeks.

**Goal:** Increase the percentage of incident engagement.

**Best practices:**
* Educate the DevOps team on when it is appropriate to acknowledge an incident alert.
* Gamify alert acknowledgement to drive usage.
* Discourage mass acknowledgement exercises.
  </Collapser>

  <Collapser
    id="kpi-mtti"
    title="Mean time to investigate (MTTI) KPI"
  >
Mean time to investigate identifies the average time it takes for an incident to be acknowledged. Typically you should compare the current and previous weeks.

**Goal:** Reduce the mean time to investigate.

**Best practices:**
* Work at building incident responder's confidence in alerts.
* Ensure that valuable alerts are acknowledged.
* Incentivize response teams to respond quickly to alerts.
  </Collapser>
</CollapserGroup>
</Collapser>
</CollapserGroup>

