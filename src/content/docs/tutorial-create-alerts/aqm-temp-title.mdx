---
title: "Manage alert quality"
metaDescription: "How to manage the quality of your alerts with New Relic"
---

Teams suffer from alert fatigue when they experience high alert volumes and alerts that aren't aligned to business impact. High alert volumes train incident responders to assume the alerts are false and have no business impact. In turn, they may start to prioritize easy-to-resolve alerts over others and they may close unresolved incidents so they can stay within their SLA targets. This results in slower incident response and increased scope and severity when true business impacting issues occur.

Alert quality management (AQM) focuses on reducing the number of nuisance incidents so that you focus only on alerts with true business impact. This reduces alert fatigue and ensures that you and your team focus your attention on the right places at the right times.

<Callout variant="tip">
Want to get your hands dirty before you start implementing this in your account? Check out the alert quality management lab
</Callout>

## Why use alert quality management?

You're a good candidate for AQM if:

* You have too many alerts.
* You have alerts that stay open for long time periods.
* Your alerts are not relevant.
* Your customers discover your issues before your monitoring tools do.
* You can't see the value of your observability tool(s).

By using an alert strategy based on measuring business impact, you'll decrease response time and increase awareness of critical events. As you improve your alert signal to noise ratio, you'll reduce confusion and be able to rapidly identify and isolate the root cause of your problems.
The overall goal of alert quality management is to ensure that fewer, more valuable, incidents are created. This will result in:
Increased uptime and availability
Reduced mean time to resolution (MTTR)
Decreased alert volume
The ability to easily identify alerts that are not valuable, so you can either make them valuable or remove them.

## Using Key Performance indicators

You'll use the AQM process to collect and measure incident volume and engagement KPIs.

<CollapserGroup>
    <Collapser
        id="incident_volume"
        title="Incident volume"
    >
You should treat incidents (with or without alerts) like a queue of tasks. Just like a queue, the number of alerts should spend time near zero. Each incident should be a trigger for an investigatory or corrective action to resolve the condition. If an alert does not result in some sort of action, then you should question the value of the alert condition.
In particular, if you see specific incidents that are "always on", then you should question why. Are you in a constant state of business impact, or do you simply have a large volume of noise? The alert volume KPIs help you to answer those questions and to help you measure progress towards a healthy state of high quality alerting.

  <Collapser
    id="kpi-incident-count"
    title="Incident count KPI"
  >
Incident count is the number of incidents generated over a period of time. Typically you should compare the current and previous weeks.

**Goal:** Reduce the number of low value / nuisance incidents.

**Best practices:**
* Ensure condition settings are intended to detect real business impact.
* Ensure condition settings are detecting abnormal behavior.
* Communicate that the incident details "Acknowledge" feature helps measure meaningful and actionable alerts. See [percentage incident acknowledge KPI](#kpi-user-engagement).
* Report AQM KPIs to all stakeholders.

```sql
FROM NrAiIncident SELECT count(*) AS 'Incident Count' WHERE event = 'open' AND priority = 'critical' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>

  <Collapser
    id="kpi-incident-duration"
    title="Accumulated incident duration KPI"
  >
Accumulated incident duration is the total sum of minutes that all incidents have accumulated over a period of time. Typically you should compare the current and previous weeks.

**Goal:** Reduce the total accumulated minutes of incidents.

**Best practices:**
* Do not manually close incidents that do not motivate some sort of investigatory action. Manual closure will skew the real duration of incident length.
* Eliminate alerts that do not result in any remediation actions from the recipients.
* Improve percent investigated and mean-time-to-investigate KPIs by communicating their importance in improving detection and response times.
* Report AQM KPIs to all stakeholders.

```sql
FROM NrAiIncident SELECT sum(durationSeconds)/60 AS 'Incident Minutes' WHERE event = 'close' AND priority = 'critical' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>

  <Collapser
    id="kpi-mttc"
    title="Mean time to close (MTTC) KPI"
  >
Average duration of incidents within the period of time measured.

**Goal:** Reduce MTTC

**Best practices:**
* Do not manually close incidents. Manual closure will skew the real duration of incident length.
* Improve reliability engineering skills.
* Report AQM KPIs to all stakeholders.

```sql
FROM NrAiIncident SELECT average(durationSeconds/60) AS 'Incident MTTC (minutes)' WHERE event = 'close' AND priority = 'critical' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>

  <Collapser
    id="kpi-pct-under-five"
    title="Percent under 5 minutes KPI"
  >
Percentage of incidents where the duration of the incident is under five minutes. This can be an indicator of incident flapping.

**Goal:** Minimize percentage of incidents with short durations.

**Best practices:**
* Ensure that conditions are detecting legitimate deviations from expected behavior.
* Understand service level management.
* Ensure that conditions are detecting legitimate deviations that correlate to business impact or impending business impact.

```sql
FROM NrAiIncident SELECT percentage(count(*), WHERE durationSeconds <= 5*60) AS '% Under 5min' WHERE event = 'close' AND priority = 'critical' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO 
```
  </Collapser>
</Collapser>
</CollapserGroup>

<CollapserGroup>
    <Collapser
        id="user_engagement"
        title="User engagement"
    >
You should measure the value of an incident by the amount of attention it receives. Here, we measure engagement by whether or not an incident has been acknowledged.

The amount of engagement an individual alert receives is a direct measurement of its value. More engagement implies a valuable alert. Less (or zero) engagement implies a nuisance alert that should be modified or disabled.

There is a significant difference between measuring the moment of incident awareness vs. acknowledging the moment resolution activity begins. If you are using an integration with New Relic alerts, be sure that the "acknowledge" event that is sent to New Relic is triggered when resolution activity begins, not when the incident is sent to the external incident management tool. 

For more information regarding standard incident management processes, see "[Incident management process: 5 steps to effective resolution, posted on August 31, 2020 by OnPage Corporation.](https://www.onpage.com/incident-management-process-5-steps-to-effective-resolution) in reference to [ITIL4](https://itsm.tools/its-here-itil-4-explained)"

<CollapserGroup>
  <Collapser
    id="kpi-pct-ack"
    title="Percentage acknowledged KPI"
  >
Incidents acknowledged identifies the percentage of incidents that have been engaged with by having their acknowledgement flag set to true. Typically you should compare the current and previous weeks.

**Goal:** Increase the percentage of incident engagement.

**Best practices:**
* Educate the DevOps team on when it is appropriate to acknowledge an incident alert.
* Gamify alert acknowledgement to drive usage.
* Discourage mass acknowledgement exercises.

```sql
FROM NrAiIssue SELECT filter(count(*), WHERE event='acknowledge')/filter(count(*), WHERE event='create')*100 AS '% Investigated' WHERE priority='CRITICAL' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>

  <Collapser
    id="kpi-mtti"
    title="Mean time to investigate (MTTI) KPI"
  >
Mean time to investigate identifies the average time it takes for an incident to be acknowledged. Typically you should compare the current and previous weeks.

**Goal:** Reduce the mean time to investigate.

**Best practices:**
* Work at building incident responder's confidence in alerts.
* Ensure that valuable alerts are acknowledged.
* Incentivize response teams to respond quickly to alerts.

```sql
FROM NrAiIssue SELECT average(acknowledgeTime - activateTime) / 60000 AS 'Incident MTTI (minutes)' WHERE event = 'acknowledge' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>
</CollapserGroup>
</Collapser>
</CollapserGroup>

## Realizing value [#value-realization]

Once the [AQM process](/docs/tutorial-create-alerts/improve-with-alerts/) is established, you will see significant reductions in the volume of alerts while reliability and stability remain the same or improve. In addition, you should see that your alerts have a clear and unambiguous business impact. Your AQM KPIs will provide quantifiable proof of these improvements.

This process should also help alert creators to better understand the impact that new alert policies put on responders and help alert creators to build more meaningful alert policies.

Once you are firmly on the path to meeting the goals of AQM, consider moving to other use cases within the **Uptime, performance, and reliability** value stream, such as [Service level management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/slm-implementation-guide), or reliability engineering. You can also move to other observability maturity value streams, such as [Customer experience](/docs/new-relic-solutions/observability-maturity/customer-experience/quality-foundation-implementation-guide).

## Migrating to AQM [#aqm-migration-guide]

The original release of AQM leveraged a custom event called nrAQMIncident to drive the process.  The nrAQMIncident event was generated by a webhook associated with each alert policy in the account and generated both incident volume and engagement KPIs.

The current version of AQM uses two default events, `NrAiIncident` and `NrAiIssue`, which are automatically generated.

The legacy webhook method used by the original release of AQM is going to be deprecated in January, 2023.  You should transition to the new methodology prior to that time. If your organization is adopting AQM for the first time, you should use the new, `NrAiIncident`, based methodology.

When you transition, you should be aware of the following:

* Your existing AQM dashboard will need to be replaced with the new AQM dashboard that uses the `NrAiIncident` / `NrAiIssue` events.
* These events are required to generate the incident volume KPIs and incident engagement KPIs.
* As you transition from `nrAQMIncident` to `NrAiIncident`, your KPI numbers will change substantially. This is because the `NrAiIncident` events track the number of times threshold violations have occurred, while the `nrAQMIncident` events tracked alert notifications.  While the numbers have changed, the underlying relationships with the KPIs and the relative values between them have not.  You will need to educate the AQM participants on this to reduce the risks of confusion.

Once your transition is complete, you can disable or delete the AQM webhook and the old dashboard.

