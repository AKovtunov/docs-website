---
title: "Manage your alert quality"
metaDescription: "How to manage the quality of your alerts with New Relic"
---

Teams suffer from alert fatigue when experiencing high alert volumes and alerts that aren't aligned to business impact. When this happens,incident responders often assume the alerts are false and have no business impact. In turn, they may start to prioritize easy-to-resolve alerts over others or close unresolved incidents to stay within their response time targets. This almost always results in slower incident response and increased scope and severity when major issues occur.

Alert quality management (AQM) focuses on reducing the number of nuisance incidents so that you focus only on alerts with true business impact. This reduces alert fatigue and ensures that you and your team focus your attention on the right places at the right times.

You're a good candidate for AQM if:

* You have too many alerts.
* You have alerts that stay open for long time periods.
* You have a lot of alerts aren't relevant.
* Your customers discover your issues before your monitoring tools do.

<Callout variant="tip">
Want to try a hands on learning approach before you start implementing this in your account? Check out the [alert quality management lab](https://learn.newrelic.com/hands-on-lab-alert-quality-management).
</Callout>

## Why use alert quality management? [#why-aqm]

By using an alert strategy based on measuring business impact, you'll decrease response time and increase awareness of critical events. As you improve your alert signal to noise ratio, you'll reduce confusion and be able to rapidly identify and isolate the root cause of your problems. The overall goal of alert quality management is to lower the volume of less valuable alerts while providing easier ways to identify when more valuable incidents are created. This results in:

* Increased uptime and availability.
* Reduced mean time to resolution (MTTR).
* Decreased alert volume.
* The ability to easily identify alerts that are not valuable, so you can either make them valuable or remove them.

AQM uses metrics to help you measure these, called **Key performance indicators**. For more on KPIs, see below.

## Using Key performance indicators [#kpi]

KPIs help you to find the noisiest and least valuable alerts so you can improve their value or remove them. You'll use the AQM process to collect and measure incident volume and engagement KPIs, and in turn, you'll use the long term metric trends to show real business impact to management and stakeholders. Below, you'll find information on all the KPIs, as well as a NRQL query for each one to help you monitor them from anywhere in the New Relic UI. 

### Incident volume [#volume]

You should treat incidents (with or without alerts) like a queue of tasks. Just like a queue, the number of alerts always be as close to zero as possible. Each incident should trigger an investigatory or corrective action to resolve the condition. If an alert doesn't result in some sort of action, then you should question the value of the alert condition.

In particular, if you see specific incidents that are frequently triggered, then you should question whether you're in a constant state of meaningful impact or if you simply have a large volume of noise? The incident volume KPIs help you answer those questions and measure progress towards a healthy state of high quality alerting.

<CollapserGroup>
  <Collapser
    id="kpi-incident-count"
    title="Incident count KPI"
  >
This is the number of incidents generated over a period of time. Typically you should compare the current and previous weeks.

**Goal:** Reduce the number of low value / nuisance incidents.

**Best practices:**
* Ensure condition settings are intended to detect real business impact.
* Ensure condition settings are detecting abnormal behavior.
* Communicate that the incident details `Acknowledge` feature helps measure meaningful and actionable alerts. See [percentage incident acknowledge KPI](#kpi-user-engagement).
* Report AQM KPIs to all stakeholders.

```sql
FROM NrAiIncident SELECT count(*) AS 'Incident Count' WHERE event = 'open' AND priority = 'critical' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>

  <Collapser
    id="kpi-incident-duration"
    title="Accumulated incident duration KPI"
  >
This is the total sum of minutes that all incidents have accumulated over a period of time. Typically you should compare the current and previous weeks.

**Goal:** Reduce the total accumulated minutes of incidents.

**Best practices:**
* Avoid manually closing incidents that don't motivate some sort of investigatory action. Manual closure will skew the real duration of incident length.
* Remove alerts that don't result in any remediation actions from the recipients.
* Improve **percent investigated** and **mean-time-to-investigate** KPIs by communicating their importance in improving detection and response times.
* Report AQM KPIs to all stakeholders.

```sql
FROM NrAiIncident SELECT sum(durationSeconds)/60 AS 'Incident Minutes' WHERE event = 'close' AND priority = 'critical' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>

  <Collapser
    id="kpi-mttc"
    title="Mean time to close (MTTC) KPI"
  >
This is simply the average duration of incidents within the period of time measured. You want this number to be as low as possible.

**Goal:** Reduce MTTC

**Best practices:**
* Don't manually close incidents. Manual closure will skew the real duration of incident length.
* Improve reliability engineering skills.
* Report AQM KPIs to all stakeholders.

```sql
FROM NrAiIncident SELECT average(durationSeconds/60) AS 'Incident MTTC (minutes)' WHERE event = 'close' AND priority = 'critical' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>

  <Collapser
    id="kpi-pct-under-five"
    title="Percent under 5 minutes KPI"
  >
This is the percentage of incidents whose total duration is under five minutes. This can be an indicator of of an incident changing state too frequently, which can obscure the cause and severity of the incident. This state is known as **incident flapping**.

**Goal:** Minimize percentage of incidents with short durations.

**Best practices:**
* Ensure that conditions are detecting legitimate deviations from expected behavior.
* Understand service level management.
* Ensure that conditions are detecting legitimate deviations that correlate to business impact or impending business impact.

```sql
FROM NrAiIncident SELECT percentage(count(*), WHERE durationSeconds <= 5*60) AS '% Under 5min' WHERE event = 'close' AND priority = 'critical' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO 
```
</Collapser>
</CollapserGroup>

### User engagement [#user]

You should measure the value of an incident by the amount of attention it receives. The amount of engagement an individual alert receives is a direct measurement of its value. More engagement implies a valuable alert, while less (or zero) engagement implies an alert may simply be noisy and should be modified or disabled.

There's a significant difference between measuring the moment of incident awareness and acknowledging when resolution activity begins. If you're using an integration with New Relic alerts, be sure that the `Acknowledge` event sent to New Relic triggers when resolution activity begins, not when the incident is sent to the external incident management tool. 

<Callout variant="tip">
For more information regarding standard incident management processes, see "[Incident management process: 5 steps to effective resolution, posted on August 31, 2020 by OnPage Corporation.](https://www.onpage.com/incident-management-process-5-steps-to-effective-resolution) in reference to [ITIL4](https://itsm.tools/its-here-itil-4-explained)"
</Callout>

<CollapserGroup>
  <Collapser
    id="kpi-pct-ack"
    title="Percentage acknowledged KPI"
  >
This identifies the percentage of incidents that you've engaged with by having their acknowledgement flag set to `true`. Typically you should compare the current and previous weeks.

**Goal:** Increase the percentage of incident engagement.

**Best practices:**
* Ensure that your DevOps team knows when it's appropriate to acknowledge an incident alert, if applicable.
* Gamify alert acknowledgement to drive usage.
* Discourage mass acknowledgement exercises.

```sql
FROM NrAiIssue SELECT filter(count(*), WHERE event='acknowledge')/filter(count(*), WHERE event='create')*100 AS '% Investigated' WHERE priority='CRITICAL' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>

  <Collapser
    id="kpi-mtti"
    title="Mean time to investigate (MTTI) KPI"
  >
This identifies the average time it takes for you to acknowledge an incident. Typically you should compare the current and previous weeks.

**Goal:** Reduce the mean time to investigate.

**Best practices:**
* Work at building incident responder's confidence in alerts.
* Ensure that valuable alerts are acknowledged.
* Incentivize response teams to respond quickly to alerts.

```sql
FROM NrAiIssue SELECT average(acknowledgeTime - activateTime) / 60000 AS 'Incident MTTI (minutes)' WHERE event = 'acknowledge' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>
</CollapserGroup>

## What's next? [#next]

Once you implement the AQM process from the [previous doc](/docs/tutorial-create-alerts/improve-with-alerts/), you'll see significant reductions in the volume of alerts while maintaining reliability and stability. Your AQM KPIs, which you can measure above, will provide quantifiable proof of these improvements.

Once you've finished implementing AQM, you can also look into improving and managing other aspects of your platform, such as:
* [Service level management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/slm-implementation-guide/)
* [Reliability engineering](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/diagnostics-beginner-guide/)
* [Customer experience](/docs/new-relic-solutions/observability-maturity/customer-experience/quality-foundation-implementation-guide/)

<UserJourneyControls
previousStep={{path: "/docs/tutorial-create-alerts/improve-with-alerts/", title: "Previous step", body: "Learn how to improve your stack with alerts"}}
/>