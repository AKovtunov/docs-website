---
title: Create alert conditions
tags:
  - Alerts and applied intelligence
  - Alerts
  - Alert conditions
translate:
  - jp
metaDescription: "Use the conditions page to identify what triggers an alert policy's notification, starting with the product and type of metric or service."
redirects:
  - /docs/alerts/new-relic-alerts-beta/configuring-alert-policies/defining-alert-conditions
  - /docs/alerts/new-relic-alerts-beta/configuring-alert-policies/define-alert-conditions
  - /docs/new-relic-alerts-jvm-health-metrics
  - /docs/new-relic-alerts-alert-apm-app-response-percentile
  - /docs/new-relic-alerts-dynamic-targeting
  - /docs/alerts/new-relic-alerts/configuring-alert-policies/define-alert-conditions
  - /docs/alerts/new-relic-alerts/defining-conditions/define-alert-conditions
  - /docs/alerts/new-relic-alerts/defining-conditions/create-alert-conditions
  - /docs/alerts-and-applied-intelligence/new-relic-alerts/alert-conditions
---

import alertsCreateConditionGuided from 'images/alerts_screenshot-full_create-condition-guided.webp'

import alertsCreateConditionOwnQuery from 'images/alerts_screenshot-full_create-condition-own-query.webp'

import alertsCreateConditionFromChart from 'images/alerts_screenshot-crop_create-condition-from-chart.webp'

import alertsFromChartSelectQuery from 'images/alerts_screenshot-crop_from-chart-select-query.webp'

import alertsFineTuneYourSignal from 'images/alerts-screenshot-crop_fine-tune-your-signal.webp'

import alertsSetConditionThresholds from 'images/alerts-screenshot-crop_set-condition-thresholds.webp'


Let's see how to define the parameters that are going to be part of a condition and get an alert that triggers an incident.

## Create a condition [#create-condition]

To create an alert condition you have these options:

<CollapserGroup>
<Collapser
    id="alert-condition-guided-mode"
    title="Create an alert condition from the guided mode"
    >


1. Go to **[one.newrelic.com](https://one.newrelic.com/all-capabilities) > Alerts & AI**, then click **Alert Conditions** on the left navigation pane.
2. Click **+New alert condition**.
3. Choose **Use guided mode**, which is the recommended option.

<img
  title="Alerts Conditions main page"
  alt="Alerts Conditions main page"
  src={alertsCreateConditionGuided}
/>

4. Select the part of the system you want to create the alert condition.
5. Click **Next**.
6. Select the entities to watch. There is a maximum of 20.
7. Select a metric to monitor. You can choose **[Golden metrics](/docs/apis/nerdgraph/examples/golden-metrics-entities-nerdgraph-api-tutorial)** or select a defined metric.
8. Click **Next**.
9. Set thresholds and adjust your signal. 
      * **Configure the [data aggregation](/docs/alerts-applied-intelligence/new-relic-alerts/get-started/choose-your-aggregation-method/)**.
      * **Gap filling strategy**. [Gap-filling](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-alert-conditions/#data-gaps) is a reactive feature that fills blank periods in data reporting with user-defined synthetic value. If you believe the data reporting will be sporadic, you can avoid false alerts by filling in empty aggregation windows with this data. For your first alert, select **None**.
      * **[Evaluation delay](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-alert-conditions/#evaluation-delay)**. If used, this option enables a period of time for New Relic to wait before evaluating a new signal against your set threshold. It's useful for preventing false positive incidents when a data stream begins to report. For your first alert, you can leave it to the disabled toggle unless you know that this option applies to your data.
      * **Set condition [thresholds](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/set-thresholds-alert-condition/)**. 

10. Click **Next**.
11. Name your alert condition. Make sure that you use a clear and meaningful name. Later, you can [modify this name](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/overview/#change-condition).
12. Connect the condition to a policy. You can choose an existing policy or [create a new one](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/create-edit-or-find-alert-policy/).
12. You can add a runbook URL so that personnel handling the incident that triggered the alert will know what to do.
13. Click **Save condition**.

</Collapser>
<Collapser
    id="alert-condition-query"
    title="Create a condition writing your own query"
    >

1. Go to **[one.newrelic.com](https://one.newrelic.com/all-capabilities) > Alerts & AI**, then click **Alert Conditions** on the left navigation pane.
2. Click **+New alert condition**.
3. Choose **Write your own query**.

<img
  title="Alerts Conditions main page"
  alt="Alerts Conditions main page"
  src={alertsCreateConditionOwnQuery}
/>

4. Write your query and click **Run** to see the chart. See [How to use NRQL: the mechanics of querying](/docs/query-your-data/nrql-new-relic-query-language/get-started/introduction-nrql-how-nrql-works/) if you need help for adding a query. 
5. Click **Next**.
6. Set thresholds and adjust your signal. 
      * **Configure the [data aggregation](/docs/alerts-applied-intelligence/new-relic-alerts/get-started/choose-your-aggregation-method/)**.
      * **Gap filling strategy**. [Gap-filling](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-alert-conditions/#data-gaps) is a reactive feature that fills blank periods in data reporting with user-defined synthetic value. If you believe the data reporting will be sporadic, you can avoid false alerts by filling in empty aggregation windows with this data. For your first alert, select **None**.
      * **[Evaluation delay](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-alert-conditions/#evaluation-delay)**. If used, this option enables a period of time for New Relic to wait before evaluating a new signal against your set threshold. It's useful for preventing false positive incidents when a data stream begins to report. For your first alert, you can leave it to the disabled toggle unless you know that this option applies to your data.
      * **Set condition [thresholds](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/set-thresholds-alert-condition/)**. 

7. Click **Next**.
8. Name your alert condition. Make sure that you use a clear and meaningful name. Later, you can [modify this name](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/overview/#change-condition).
9. Connect the condition to a policy. You can choose an existing policy or [create a new one](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/create-edit-or-find-alert-policy/).
10. You can add a runbook URL so that personnel handling the incident that triggered the alert will know what to do.
11. Click **Save condition**.

See [NRQL and alert conditions]() if you want to know more about it.

</Collapser>

<Collapser
    id="alert-condition-from-chart"
    title="Create a condition from a chart"
    >
Most of our charts, with the exception of a few older ones, allow you to create a condition from them.

To create an alert condition from a chart, click the <Icon name="fe-more-horizontal"/> icon on the right corner and select **Create alert condition**.

<img
  width="70%;"
  title="Create a condition from a chart"
  alt="Create a condition from a chart"
  src={alertsCreateConditionFromChart}
/>

If the chart includes multiple queries, you'll have the option of selecting one of them to create an alert on your selected query.

<img
  width="40%;"
  title="Create a condition from a chart, selecting a query"
  alt="Create a condition from a chart, selecting a query"
  src={alertsFromChartSelectQuery}
/>


</Collapser>
</CollapserGroup>

<Callout variant="important">
  Conditions that provide fields for you to input numerical values accept decimal points up to the second decimal place (hundredths). For example, `0.01` is the smallest possible value.
</Callout>

## Advanced signal settings [#advanced-signal]

When creating a condition, there are several advanced signal settings:

* Aggregation window duration
* Sliding window aggregation
* Streaming method
* Delay/timer
* Fill data gaps
* Evaluation delay

<img
  width="50%;"
  title="Screenshot showing the advanced signal settings"
  alt="Screenshot showing the advanced signal settings"
  src={alertsFineTuneYourSignal}
/>

<figcaption>
  Advanced signal settings in the creation of an alarm condition. Go to **[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Alerts & AI > Alert conditions**, then **+ New alert condition**.
</figcaption>

To read an explanation of what these settings are and how they relate to each other, see [Streaming alerts concepts](/docs/alerts-applied-intelligence/new-relic-alerts/get-started/streaming-alerts-key-terms-concepts). Below are instructions and tips on how to configure them.

### Aggregation window duration [#window-duration]

You can set the [aggregation window duration](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/#window-duration) to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 120 minutes. The default is one minute.

### Sliding window aggregation [#sliding-window-aggregation]

You can use [sliding windows](/docs/query-your-data/nrql-new-relic-query-language/nrql-query-tutorials/create-smoother-charts-sliding-windows) to create smoother charts. This is done by creating overlapping windows of data.

Learn how to set sliding windows in this short video (2:30 minutes):

<Video
  id="-5--8DZynFE"
  type="youtube"
/>

Once enabled, set the "slide by interval" to control how much overlap time your aggregated windows have. The interval must be shorter than the aggregation window while also dividing evenly into it.

<Callout variant="important">
  Immediately after you create a new sliding windows alert condition or perform any action that can cause an [evaluation reset](#evaluation-resets), your condition will need time build up an "aggregated buffer" for the duration of the first aggregation window. During that time, no incidents will trigger. Once that single aggregation window has passed, a complete "buffer" will have been built and the condition will function normally.
</Callout>

### Streaming method [#streaming]

Choose between [three streaming aggregation methods](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/#aggregation-methods) to get the best evaluation results for your conditions.

### Delay/timer [#delay-timer]

You can adjust the [delay/timer](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/#delay-timer) to coordinate [our streaming alerting algorithm](/docs/new-relic-solutions/get-started/glossary/#streaming-algorithm) with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method.

For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay.

If the data type comes from an [APM language agent](/docs/apm/new-relic-apm/getting-started/introduction-apm) and is aggregated from many app instances (for example, `Transactions`, `TransactionErrors`, etc.), we recommend using the event flow method with the default settings.

<Callout variant="important">
  When creating NRQL conditions for data collected from [Infrastructure Cloud Integrations](/docs/infrastructure/infrastructure-integrations/get-started/introduction-infrastructure-integrations/#cloud) such as AWS CloudWatch or Azure, we recommend that you use the event timer method.
</Callout>

### Fill data gaps [#data-gaps]

Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings:

* **None**: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be an incident.
* **Custom static value**: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of `fillValue` (as named in the API) that specifies what static value should be used. This defaults to `0`.
* **Last known value**: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for a minimum of 2 hours. If the configured threshold duration is longer than 2 hours, this value is kept for that duration instead.

<Callout variant="tip">
  The alerts system fills gaps in actively reported signals. This signal history is dropped after a period of inactivity and, for gap filling, data points received after this period of inactivity are treated as new signals. The inactivity length is either 2 hours or the configured threshold duration, whichever is longer.

  To learn more about signal loss, gap filling, and how to request access to these features, see [this Support Forum post](https://discuss.newrelic.com/t/announcing-new-relic-one-streaming-alerts-for-nrql-conditions/115361).
</Callout>

Options for editing data gap settings:

* In the NRQL conditions UI, go to **Condition settings > Advanced signal settings > fill data gaps with** and choose an option.
* If using our [Nerdgraph API](/docs/alerts-applied-intelligence/new-relic-alerts/alerts-nerdgraph/nerdgraph-api-loss-signal-gap-filling) (preferred), this node is located at:
  `actor : account : alerts : nrqlCondition : signal : fillOption | fillValue`
* NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the **"signal"** section of [the Alert NRQL conditions API](https://rpm.newrelic.com/api/explore/alerts_nrql_conditions/list).

### Evaluation delay [#evaluation-delay]

You can enable the `Use evaluation delay` flag and set up to 120 minutes to delay the evalution of incoming signals.

When new entities are first deployed, resource utilization on the entity is often unusually high. In autoscale environments this can easily create a lot of false alerts. By delaying the start of alert detection on signals emitted from new entities you can significantly reduce the number of false alarms associated with deployments in orchestrated or autoscale environments.

Options to enable evaluation delay:

* In the NRQL conditions UI, go to **Adjust to signal behavior > Use evaluation delay**.
* If using our [Nerdgraph API](/docs/apis/nerdgraph/examples/nerdgraph-api-nrql-condition-alerts/#static-condition), this node is located at:
  `actor : account : alerts : nrqlCondition : signal : evaluationDelay`


## Set the loss of signal threshold [#signal-loss]

Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed.

<Callout variant="important">
  The loss of signal feature requires a signal to be present before it can detect that the signal is lost. If you enable a condition while a signal is not present, no loss of signal will be detected and the loss of signal feature will not activate.
</Callout>

<img
  width="50%;"
  title="Screenshot showing the set condition thresholds settings"
  alt="Screenshot showing the set condition thresholds settings"
  src={alertsSetConditionThresholds}
/>

<figcaption>
  Set condition thresholds settings in the creation of an alarm condition. Go to **[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Alerts & AI > Alert conditions**, then **+ New alert condition**.
</figcaption>

You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific [GraphQL API examples](/docs/alerts-applied-intelligence/new-relic-alerts/alerts-nerdgraph/nerdgraph-api-loss-signal-gap-filling).

**Loss of signal settings:**

Loss of signal settings include a time duration and two possible actions.

* **Signal loss expiration time**
  * UI label: **Signal is lost after:**
  * GraphQL Node: [expiration.expirationDuration](/docs/apis/nerdgraph/examples/nerdgraph-api-loss-signal-gap-filling/#loss-of-signal)
  * Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the `WHERE` clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal.
  * The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires.
  * The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes.
* **Loss of signal actions**
  Once a signal is considered lost, you can close open incidents, open new incidents, or both.
  * Close all current open incidents: This closes all open incidents that are related to a specific signal. It won't necessarily close all incidents for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that incidents are closed properly. The GraphQL node name for this is ["closeViolationsOnExpiration](/docs/apis/nerdgraph/examples/nerdgraph-api-loss-signal-gap-filling/#loss-of-signal)"
  * Open new incidents: This will open a new incident when the signal is considered lost. These incidents will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is ["openViolationOnExpiration](/docs/apis/nerdgraph/examples/nerdgraph-api-loss-signal-gap-filling/#loss-of-signal)"
  * When you enable both actions, we'll close all open incidents first, and then open a new incident for loss of signal.

Incidents open due to loss of signal close when:

* the signal comes back. Newly opened lost signal incidents will close immediately when new data is evaluated.
* the condition they belong to expires. By default, conditions expire after 3 days.
* you manually close the incident with the **Close all current open incidents** option.

<Callout variant="tip">
  Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries.
</Callout>


