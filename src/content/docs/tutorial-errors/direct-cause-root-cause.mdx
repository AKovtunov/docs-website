---
title: Find the root cause
metaDescription: Learn how to manage a high number of errors in your app.   
---

In the last tutorial, we covered how New Relic organizes data about your errors so you can identify patterns in your error groups. By leveraging stack traces and logs for your error analysis, you can find the source of your error. But this doesn't mean you've found the root cause. 

Let's think back to our flooding yard example. We've talked about why the source of the flooding doesn't necessarily mean it's the cause of the flooding -- it's just the place that's indicating a problem. The busted pipe in your water main is your direct cause, but it isn't the root cause of the flooding. Rather, the root cause is improper installation, or perhaps the type of pipe chosen. 

While you've patched the cracked pipe, if you continue using that same company, you haven't really addressed the root cause of an error. New Relic can help you piece together data points so you and your team can resolve an error, determine the cause of that error, and address the problem at its root. 

## Objectives [#objectives]

This tutorial covers the following tasks:

* Defining the problem behind an error
* Identifying the source
* Finding the direct cause
* Understanding root cause

## Define the problem [#define]

There are two primary experiences that impact an end-user's perception of the performance of your digital product and its capabilities. The terms below are from the customer's perspective using common customer language.


Availability is also known as: connectivity, uptime, reachability. But it's also conflated with success (non-errors).

An end-user may state that they cannot access a required capability, such as login, browse, search, view inventory. Or they may simply state that the entire service is unavailable. This is a symptom of either the inability to connect to a service or a service returning an error.

Traditionally "availability" or "uptime" was measured in a binary "UP/DOWN" methodology by measuring the ability to connect to a service. The traditional method has a critical gap in that it only measures when an entire service becomes completely unavailable. This classic measure of reliability results in significant observability gaps, difficult diagnostics, and the end-users being significantly impacted before you can react.

Availability is measured by both "the ability to reach a service", also known as "uptime", AND "the ability of the service to return the expected response," (in other words, "non-error"). New Relic's observability maturity framework distinguishes the two by input performance (connectivity) and output performance (success and latency of the response).

Performance is also known as: latency, and response time.

An end user may state that the service is too slow.

For both the IT and business leaders, the term "performance" can encompass an array of issues. In New Relic's service level management, "slowness" is measured in both the "output" and "client" categories. However, the majority of slowness problems occur due to an output issue, stemming from what are traditionally called the "backend services."


## Pending title

### Step 1: Define the problem [#create-problem-statement]

The first rule is to quickly establish the problem statement. There are plenty of guides on building problem statements but simple and effective is the best. A well formed problem statement will do the following:

1. Describe what the end-user is experiencing. What is the problem that the end-user is experiencing?
2. Describe the expected behavior of the product capability. What is it that the end-user should be experiencing?
3. Describe the current behavior of the product capability. What is the technical assessment of what the user is experiencing?

Avoid any assumptions in your problem statement. Stick to the facts.

### Step 2: Find the source [#find-source]

The "source" is the component or code that is closest to the direct cause of the problem.

Think of many water pipes connected through many junctions, splitters, and valves. You're alerted that your water output service level is degraded. You trace the problem from the water output through the pipes until you determine which junction, split, valve, or pipe is causing the problem. You discover one of the electric valves is shorted. That valve is the source of your problem. The short is the direct cause of your problem. You easily resolve the direct cause by replacing the value. Bear in mind the root cause may be something more complex, such as weather conditions, chemicals in the water, or the manufacture.

This is the same concept for diagnosing complex technology stacks. If your login capability is limited (output), you must trace the problem back to the component (source) that's causing that limit and fix it. It could be the API software (service boundary), the middleware services, the database, resource constraints, a third party service, or something else.

In IT there are three primary breakpoint categories to improve your response times:

1. **Output**
2. **Input**
3. **Client**

Defining your performance metrics within these categories, a.k.a [service levels](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide), will significant improve your response time in determining where the source of the problem is. Measuring these categories is covered in [our service level management guide](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide). To understand how to use them in diagnostics, keep reading.

### Step 3: Find the direct cause [#find-direct-cause]

Once you're close to the source of the problem, determine what changed. This will help you quickly determine how to immediately resolve the problem in the short term. In the example in [Step 2](#find-source), the change was that the valve was no longer functioning due to degraded hardware causing a short.

Examples of common changes in IT are:

1. Throughput (traffic)
2. Code (deployments)
3. Resources (hardware allocation)
4. Upstream or downstream dependency changes
5. Data volume

For other common examples of performance-impacting problems, see the [problem matrix](#problem-matrix) below.

## Gather your data points [#gather]

<CollapserGroup>
  <Collapser
    id="output-perf"
    title="Output performance"
  >

**This requires**: APM

Output performance is the ability of your internal technology stack to deliver the expected responses (output) to the end-user. This is traditionally referred to as the "backend" services.

In the great majority of scenarios, output performance is measured simply by the speed of the response and the quality of the response (in other words, it's error free). Remember the user perspective described above. The end-user will state that the service is either slow, not working, or inaccessible.

The most common problem is the ability to respond to end-user requests in a timely **and** successful manner.

This is easily identified by a latency anomaly or error anomaly on the services that support the troubled product capability.
</Collapser>
  <Collapser
    id="input-perf"
    title="Input performance"
  >

**This requires**: Synthetics

Input performance is simply the ability of your services to receive a request from the client. This is not the same as the client's ability to send a request.

Your output performance (backend services) could be exceeding expected performance levels. However, something between the client and your services is breaking the request-response lifecycle. This could be anything between the client and your services.
</Collapser>
  <Collapser
    id="client-perf"
    title="Client performance"
  >
**This requires**: Browser monitoring and/or mobile monitoring

Client performance is the ability for a browser and/or mobile application to both make requests and render responses. Browser and/or mobile are easily identified as the source of the problem once both output (backend) and input performance (synthetics) have been ruled out.

Output and input performance is relatively easy to rule out (or rule in). Due to the depth of diagnostics in input and output diagnostic, browser and mobile will be covered in an advanced diagnostics guide in the future.
</Collapser>
</CollapserGroup>

## Problem matrix [#problem-matrix]

The problem matrix is a cheat-sheet of common problems categorized by the three health data points.

The problem sources are arranged by how common they are, with the most common being in the top row and on the left. A more detailed breakdown is listed below. Service level management done well will help you rule out two out of three of these data points quickly.

This table is a problem matrix sorted by health data point:

| Data point  | New Relic capability  | Common problem sources  |
|---|---|---|
| Output  | APM, infra, logs, NPM  | Application, data sources, hardware config change, infrastructure, internal networking, third party provider (AWS, GCP)  |
| Input  | Synthetic, logs  | External routing (CDN, gateways, etc), internal routing, things on the internet (ISP, etc.)  |
| Client  |  Browser, mobile |  Browser or mobile code |

Problems tend to be compounded but the goal is to "find the source" and then determine "what changed" in order to quickly restore service levels.

### Example problems [#example-problem]

Let's look at an example problem. Let's say your company deploys a new product, and the significant increase in requests causes unacceptable response times. The source is discovered in the login middleware service. The problem is a jump in TCP queue times.

Here's a breakdown of this situation:

* **Category**: output performance
* **Source**: login middleware
* **Direct cause**: TCP queue times from additional request load
* **Solution**: increased TCP connection limit and scaled resources
* **Root-cause**: insufficient capacity planning and quality assurance testing on downstream service impacting login middleware

### Another example problem [#example-problem-2]

Here's another example problem:

* There was a sudden increase in 500 Gateway errors on login...
* The login API response times increased to the point where timeouts began...
* The timeouts were traced to the database connections in the middleware layer...
* Transaction traces revealed signficant increase in number of database queries per login request...
* A deployment marker was found for a deployment that happened right before the problem.

Here's a breakdown of this situation:

* **Category**: output performance degredation leading to input performance failure
* **Source**: middleware service calling database
* **Direct cause**: 10x increased database queries after code deployment
* **Solution**: deployment rollback
* **Root-cause**: insufficient quality assurance testing

<UserJourneyControls
    nextStep={{path: "/docs/tutorial-error/analyze-your-errors", title: "Next step", body: "Direct cause versus root cause"}}
    previousStep={{path: "/docs/tutorial-error/analyze-your-errors", title: "Next step", body: "Direct cause versus root cause"}}
/>
