---
title: Scope the problem
metaDescription: Learn how to monitor your system so you can quickly identify and resolve many error occurrences fast. 
---

import allEntitiesView from 'images/apm_screenshot-crop_all_entities.webp'

import apiGateway from 'images/apm_screenshot-crop_api-gateway-summary.webp'

import apmErrorsInboxPage from 'images/apm_screenshot-crop_errors-inbox-page.webp'

import apmErrorsAnomaly from 'images/apm_screenshot-crop_errors-anomaly.webp'

INTRO TO ERRORS 

Let's think about a house with a flooded yard. Your first indication a problem exists is the excess water by your hose bib, but if you assumed the flooding is caused by a leaking hose, you'd be making an expensive mistake. With some investigation, you'll eventually find that the cause of your flooded yard is a crack in your water main. That crack is your direct cause and you need to fix it ASAP, but it isn't your root cause. With a little more investigation, you might find that the oaks in your front yard have grown into your pipe upstream. You might have patched the crack, but to prevent future problems you'll need to remove the oaks in your front yard. Those oaks are your root cause. 

This tutorial series guides you through prioritizing errors in your app. You'll learn how to define a problem and how to prioritize one error group over others so you can quickly begin the error analysis process.   

## Objectives [#objective]

This tutorial covers these concepts:

* Defining the problem
* Finding the source of an error 

## Choose between multiple alerting service [#choose]

When you're notified about an outage, you might encounter multiple kinds of error groups. Limited by time, you need to quickly evaluate the scope of the problem and make a judgment call about where you'll start analysis. It's likely that the services that alerted you aren't actually the source of the problem, only indicators that one exists. 

At this stage, you don't have enough context to make a decision about where to start. We recommend that you pivot your perspective away from your system and think about the customer experience, as this will help you gauge an error group's urgency. For example, if two services are alerting — one for logging in and one for searching inventory — but you're only receiving reports that end users can't search for products, then you should start with the error group   closest to the service making those API calls. 

When working off error groups, here are some questions that will help you view your app from a customer point of view: 

* Is the end user experiencing a problem? 
* How should their experience look?
* What behavior are they currently experiencing?

Understanding the nature of a problem is a small but important part of upfront discovery. Knowing the end effect of a failure can help parse through all the data that's being reported to New Relic. You may believe that service A is more important than service B, but until you verify the end user experience, all you're left with is an assumption.    

Let's see how this might look in practice. The below screenshot displays all entities associated with a New Relic account. Four are alerting.

<img
title="Overview errors affecting your services"
alt="A screenshot showing an app with many errors"
src={allEntitiesView}
/>

According to customer reports, you know that: 

* The end user is struggling with purchase actions.
* Your site should only display in-stock items.
* Your site is displaying all products, so customers are able to purchase out-of-stock items.

Having worked out the nature of the problem, you know that `api-gateway` is a dependency for other services that keep track of your inventory. This is the start of your observability journey. 

## Prioritize between error groups [#source] 

Now that you've chosen an entry point into the problem, you can investigate the kinds of errors affecting your app. From the `apiGateway` summary page, you'll want to first look at the **Errors** tab, which filters your data to an errors-only view. 

<img
title="Overview errors affecting your services"
alt="A screenshot showing an app with many errors"
src={apmErrorsInboxPage}
/>

For this one service, you have at least six error groups reporting with anywhere from a dozen to thousands of occurrences in your app. Although daunting at first glance, you don't actually need to sort through every since error occurrence. New Relic has grouped individual occurrences of errors for you already, but how do you go about investigating the important errors? 

Start with the time series on the far right of the waterfall view. At first, this may not look like it's giving you any granular information, but what it is telling you is when an error occurred in time. We'll break this down:

* Based on number of occurrences alone, your first instinct might tell you to start with `ActivemModel:::ValidationError` as it has 4k occurrences. If you look at the time series, though, its peaks and troughs are relatively consistent. This could be an expected error, but let's look at the other five. 
* The `Net::OpenTimeOut` error group has a similar pattern, and it actually makes up four of the six reporting groups. Across each error group, you can see consistent peaks and troughs that extend before the incident. With the same name and similar patterns, we can infer this is an expected error and can eliminate all of them. This doesn't represent a change. 
* Our last option is `JsonapiClient:::Notfound`. Like `ActivemModel:::ValidationError`, it has a distinct shape and is consistently reporting. While it doesn’t have many occurrences, the timeseries is anomalous enough that might be worth digging a bit deeper. 

To choose which error to start with, you need to adjust the time parameter. This next screenshot extends the timeseries include behavior from the last 12 hours:

<img
title="Overview errors affecting your services"
alt="A screenshot showing an app with many errors"
src={apmErrorsAnomaly}
/>

With the adjustment, you can see that `ActivemModel:::ValidationError`has an unchanging pattern of peaks and troughs, but your  `JsonapiClient:::Notfound` has a dramatic change in behavior. This is a good ground zero. 

Knowing when something happened is a critical piece for getting closer to the source. Having a complete understanding of the problem space, we can now dig into the source. 

<UserJourneyControls
    nextStep={{path: "/docs/tutorial-errors/analyze-your-errors", title: "Next step", body: "Find what changed"}}
/>

<UserJourneyControls
    previousStep={{path: "/docs/tutorial-errors/triage-your-errors", title: "Previous step", body: "Triage your errors"}}
/>
