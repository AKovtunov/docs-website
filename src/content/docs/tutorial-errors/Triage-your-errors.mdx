---
title: Maintain your errors
metaDescription: Learn how to manage a high number of errors in your app.   
---

In the last tutorial, we covered how New Relic organizes data about your errors so you can identify patterns in your error groups. By leveraging stack traces and logs for your error analysis, you can find the source of your error. But this doesn't mean you've found the root cause. 

Let's think back to our flooding yard example. We've talked about why the source of the flooding doesn't necessarily mean it's the cause of the flooding -- it's just the place that's indicating a problem. The busted pipe in your water main is your direct cause, but it isn't the root cause of the flooding. Rather, the root cause is improper installation, or perhaps the type of pipe chosen. 

While you've patched the cracked pipe, if you continue using that same company, you haven't really addressed the root cause of an error. New Relic can help you piece together data points so you and your team can resolve an error, determine the cause of that error, and address the problem at its root. 

## Objectives [#objectives]

This tutorial covers the following tasks:

* Defining the problem behind an error
* Identifying the source
* Finding the direct cause
* Understanding root cause

## Find the errors tab [#errors-inbox]

Let's say you've been notified that one of your services has gone down, and it's affecting a critical part of your customer funnel. When you check New Relic, you see that one of your services is alerting in the red, but there's also a lot of other data to look at. Your throughput might be up, or maybe there's an uptick in your error rate.

[SCREENSHOT HERE OF ERRORS EXAMPLE]

Keep in mind that when a service alerts you, it's only alerting you that a problem is affecting that particular service. The source of the error could be elsewhere. Think of it this way: you may see flooding at the side of your house near your hose and maybe that is the source of the problem. But maybe the issue is with your water main. If you assume that the flooding has to be related to the hose without further investigation, you risk wasting valuable time and money fixing a problem that doesn't exist. Worse, the crack in your water main that's causing the flooding could extend further up the line.

Once you're aware of the problem, you'll find yourself on that service's APM summary page. In this example, we're on the summary page for `API-gateway`. You may notice three things:

* Something about data
* Something about data
* Something about data

Your first step is to check the **Errors** tab under **Triage**.

[SCREENSHOT HERE OF ERRORS EXAMPLE]

## Analyze your errors [#analyze]

The errors page has two sections to begin analysis, both with the same information. The **Triage** section lets you assign errors to people or teams. The **Group errors** page gives you a waterfall view of individual errors that make up group errors. Either page summarizes the same data.

[IMAGE OF TWO ERROR GROUPS ONE EXPECTED ONE UNEXPECTED]

Your errors are categorized into two categories: expected or unexpected errors. At first glance, this might seem counter intuitive, but when you're reacting to an incident like a service outage, you want to focus on your unexpected errors. These kinds of errors are anomalous and may be due to a deployment or a dependency, rather than user error or a noncritical bug in your system. 

We recommend you take these steps when analyzing your errors:

1. Compare the different errors between the the error rate and error count graphs. Even with a high error count, if its count extends beyond the incident time frame, then you can assume it's an expected error. 
2. Adjust the time picker with a custom time frame when the initial spike in errors occurred.
3. Click the error group you want to focus on. This will bring up another page with details about logs, stack traces, distributed traces, and error attributes relating to the error occurrence.

While there's no hard and fast rule about which kind of data you use to troubleshoot, we do recommend you start with either stack traces or logs.

* If the error is on the code-level, a stack trace can point you to lines within your code that may be the source of the problem. For example [EXAMPLE HERE]
* Logs can give you an error description, such as a failed call to a dependency that triggered your service to alert. 

How you choose to troubleshoot an error depends on your familiarity with your app or services, the language your app is in, or whether or not you're in a distributed environment. Whatever the case, you can use this error summary page to sort through additional context so you can start resolving the issue. 

## Triage your error [#triage]

[SCREENSHOT OF TRIAGE HERE]

TRANSCRIBE MEDA CONVO ABOUT TRIAGE HERE

## What's next [#next]

This tutorial covered the first phase in addressing errors in your app. It guided you through how to identify a source error and triage that error to the correct teams. The next tutorial docs in this series will cover differentiating between direct cause and root cause and how to solve a high number of errors. 

## sjdfolfsdf

There are two primary experiences that impact an end-user's perception of the performance of your digital product and its capabilities. The terms below are from the customer's perspective using common customer language.


Availability is also known as: connectivity, uptime, reachability. But it's also conflated with success (non-errors).

An end-user may state that they cannot access a required capability, such as login, browse, search, view inventory. Or they may simply state that the entire service is unavailable. This is a symptom of either the inability to connect to a service or a service returning an error.

Traditionally "availability" or "uptime" was measured in a binary "UP/DOWN" methodology by measuring the ability to connect to a service. The traditional method has a critical gap in that it only measures when an entire service becomes completely unavailable. This classic measure of reliability results in significant observability gaps, difficult diagnostics, and the end-users being significantly impacted before you can react.

Availability is measured by both "the ability to reach a service", also known as "uptime", AND "the ability of the service to return the expected response," (in other words, "non-error"). New Relic's observability maturity framework distinguishes the two by input performance (connectivity) and output performance (success and latency of the response).

Performance is also known as: latency, and response time.

An end user may state that the service is too slow.

For both the IT and business leaders, the term "performance" can encompass an array of issues. In New Relic's service level management, "slowness" is measured in both the "output" and "client" categories. However, the majority of slowness problems occur due to an output issue, stemming from what are traditionally called the "backend services."


## Pending title

### Step 1: Define the problem [#create-problem-statement]

The first rule is to quickly establish the problem statement. There are plenty of guides on building problem statements but simple and effective is the best. A well formed problem statement will do the following:

1. Describe what the end-user is experiencing. What is the problem that the end-user is experiencing?
2. Describe the expected behavior of the product capability. What is it that the end-user should be experiencing?
3. Describe the current behavior of the product capability. What is the technical assessment of what the user is experiencing?

Avoid any assumptions in your problem statement. Stick to the facts.

### Step 2: Find the source [#find-source]

The "source" is the component or code that is closest to the direct cause of the problem.

Think of many water pipes connected through many junctions, splitters, and valves. You're alerted that your water output service level is degraded. You trace the problem from the water output through the pipes until you determine which junction, split, valve, or pipe is causing the problem. You discover one of the electric valves is shorted. That valve is the source of your problem. The short is the direct cause of your problem. You easily resolve the direct cause by replacing the value. Bear in mind the root cause may be something more complex, such as weather conditions, chemicals in the water, or the manufacture.

This is the same concept for diagnosing complex technology stacks. If your login capability is limited (output), you must trace the problem back to the component (source) that's causing that limit and fix it. It could be the API software (service boundary), the middleware services, the database, resource constraints, a third party service, or something else.

In IT there are three primary breakpoint categories to improve your response times:

1. **Output**
2. **Input**
3. **Client**

Defining your performance metrics within these categories, a.k.a [service levels](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide), will significant improve your response time in determining where the source of the problem is. Measuring these categories is covered in [our service level management guide](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide). To understand how to use them in diagnostics, keep reading.

### Step 3: Find the direct cause [#find-direct-cause]

Once you're close to the source of the problem, determine what changed. This will help you quickly determine how to immediately resolve the problem in the short term. In the example in [Step 2](#find-source), the change was that the valve was no longer functioning due to degraded hardware causing a short.

Examples of common changes in IT are:

1. Throughput (traffic)
2. Code (deployments)
3. Resources (hardware allocation)
4. Upstream or downstream dependency changes
5. Data volume

For other common examples of performance-impacting problems, see the [problem matrix](#problem-matrix) below.

## Gather your data points [#gather]

<CollapserGroup>
  <Collapser
    id="output-perf"
    title="Output performance"
  >

**This requires**: APM

Output performance is the ability of your internal technology stack to deliver the expected responses (output) to the end-user. This is traditionally referred to as the "backend" services.

In the great majority of scenarios, output performance is measured simply by the speed of the response and the quality of the response (in other words, it's error free). Remember the user perspective described above. The end-user will state that the service is either slow, not working, or inaccessible.

The most common problem is the ability to respond to end-user requests in a timely **and** successful manner.

This is easily identified by a latency anomaly or error anomaly on the services that support the troubled product capability.
</Collapser>
  <Collapser
    id="input-perf"
    title="Input performance"
  >

**This requires**: Synthetics

Input performance is simply the ability of your services to receive a request from the client. This is not the same as the client's ability to send a request.

Your output performance (backend services) could be exceeding expected performance levels. However, something between the client and your services is breaking the request-response lifecycle. This could be anything between the client and your services.
</Collapser>
  <Collapser
    id="client-perf"
    title="Client performance"
  >
**This requires**: Browser monitoring and/or mobile monitoring

Client performance is the ability for a browser and/or mobile application to both make requests and render responses. Browser and/or mobile are easily identified as the source of the problem once both output (backend) and input performance (synthetics) have been ruled out.

Output and input performance is relatively easy to rule out (or rule in). Due to the depth of diagnostics in input and output diagnostic, browser and mobile will be covered in an advanced diagnostics guide in the future.
</Collapser>
</CollapserGroup>

## Problem matrix [#problem-matrix]

The problem matrix is a cheat-sheet of common problems categorized by the three health data points.

The problem sources are arranged by how common they are, with the most common being in the top row and on the left. A more detailed breakdown is listed below. Service level management done well will help you rule out two out of three of these data points quickly.

This table is a problem matrix sorted by health data point:

| Data point  | New Relic capability  | Common problem sources  |
|---|---|---|
| Output  | APM, infra, logs, NPM  | Application, data sources, hardware config change, infrastructure, internal networking, third party provider (AWS, GCP)  |
| Input  | Synthetic, logs  | External routing (CDN, gateways, etc), internal routing, things on the internet (ISP, etc.)  |
| Client  |  Browser, mobile |  Browser or mobile code |

Problems tend to be compounded but the goal is to "find the source" and then determine "what changed" in order to quickly restore service levels.

### Example problems [#example-problem]

Let's look at an example problem. Let's say your company deploys a new product, and the significant increase in requests causes unacceptable response times. The source is discovered in the login middleware service. The problem is a jump in TCP queue times.

Here's a breakdown of this situation:

* **Category**: output performance
* **Source**: login middleware
* **Direct cause**: TCP queue times from additional request load
* **Solution**: increased TCP connection limit and scaled resources
* **Root-cause**: insufficient capacity planning and quality assurance testing on downstream service impacting login middleware

### Another example problem [#example-problem-2]

Here's another example problem:

* There was a sudden increase in 500 Gateway errors on login...
* The login API response times increased to the point where timeouts began...
* The timeouts were traced to the database connections in the middleware layer...
* Transaction traces revealed signficant increase in number of database queries per login request...
* A deployment marker was found for a deployment that happened right before the problem.

Here's a breakdown of this situation:

* **Category**: output performance degredation leading to input performance failure
* **Source**: middleware service calling database
* **Direct cause**: 10x increased database queries after code deployment
* **Solution**: deployment rollback
* **Root-cause**: insufficient quality assurance testing

<UserJourneyControls
    nextStep={{path: "/docs/tutorial-error/analyze-your-errors", title: "Next step", body: "Direct cause versus root cause"}}
    previousStep={{path: "/docs/tutorial-error/analyze-your-errors", title: "Next step", body: "Direct cause versus root cause"}}
/>
