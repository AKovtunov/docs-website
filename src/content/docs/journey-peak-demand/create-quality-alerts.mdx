---
title: "Reduce noise with quality alerts"
metaDescription: "Elephant."
---

Redundant alerts are distracting and run the risk of burying critical incidents under noise. When you're preparing for gameday, ensuring your alerts are lean with relevant thresholds can focus your team and prevent them from following red herrings when critical business transactions are down. By reviewing your alert policies for quality, you'll improve your team's performance and ensure that critical incidents are resolved before the business notices any impact.

This tutorial assumes you have active alerts. It'll offer some recommendations about managing the quality of your alerts. You will:

* Set up an alert quality dashboard (AQM) to review leading up to an event day
* Create new alerts with recommended NRQL strings

## Evaluate your alert policies with the AQM dashboard [#install-dashboard]

The AQM dashboard automatically evaluates your alert policies for high incident counts, high cumulative incident durations, long MTT-close, or a high percentage of incidents open for less than 5 minutes. In general, you should edit or remove policies that:

* That generate "always-on" incidents (i.e. incidents with thousands of minutes or more of cumulative duration).
* Where 30% or more of incidents are open for less than 5 minutes.
* Whose MTT-close is longer than 30 minutes.
* That create more than 350 incidents per week.

To begin, install the AQM dashboard via our quickstart:

1. [Go to the **Alert Quality Management** instant observability page.](https://newrelic.com/instant-observability/alert-quality-management)
1. Click on the **Install now** button.
1. Follow the prompts to choose the account you want to install the dashboard into.
1. View your dashboard.

We recommend you spend at least two weeks with the AQM dashboard. When preparing, your team should know that existing alert policies are under review and that acknowledging alert incidents is how you're determining alert value. For example, during the evaluation period, have your team follow these guidelines:

* If you look at an alert and decide to take any sort of further investigative action, acknowledge the alert.
* If you typically close an alert without doing anything else, do not acknowledge the alert.
* If the incident alert is always on, do not close or acknowledge it.

## Create new alerts for peak demand [#new alerts]

With your existing policies under review, you may want to create new alerts that are adjusted for peak demand. In general, we recommend attaching new alert policies for any new service levels made specific to gameday. For example:

ASK ANDREW FOR EXAMPLE THAT CARRIES OVER FROM SERVICE level

To review, here are common NRQL strings for alert conditions:

Here are some common use cases for NRQL conditions. These queries will work for static and anomaly [condition types](#threshold-types).

<CollapserGroup>
  <Collapser
    id="constrained-alerts"
    title="Alert on specific segments of your data"
  >
    Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the `WHERE` clause to define those conditions.

    ```sql
    SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230)
    ```

    ```sql
    SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%'
    ```
  </Collapser>

  <Collapser
    id="nth-percentile"
    title="Alert on Nth percentile of your data"
  >
    Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately.

    ```sql
    SELECT percentile(duration, 95) FROM Transaction
    ```

    ```sql
    SELECT percentile(databaseDuration, 75) FROM Transaction
    ```
  </Collapser>

  <Collapser
    id="max-min-avg"
    title="Alert on max, min, avg of your data"
  >
    Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold.

    ```sql
    SELECT max(duration) FROM Transaction
    ```

    ```sql
    SELECT average(duration) FROM Transaction
    ```
  </Collapser>

  <Collapser
    id="percentage"
    title="Alert on a percentage of your data"
  >
    Create alerts when a proportion of your data goes above or below a certain threshold.

    ```sql
    SELECT percentage(count(*), WHERE duration > 2) FROM Transaction
    ```

    ```sql
    SELECT percentage(count(*), WHERE http.statusCode = '500') FROM Transaction
    ```
  </Collapser>

  <Collapser
    id="apdex"
    title="Alert on Apdex with any T-value"
  >
    Create alerts on [Apdex](/docs/apm/new-relic-apm/apdex/apdex-measuring-user-satisfaction), applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8.

    ```sql
    SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%'
    ```
  </Collapser>
</CollapserGroup>