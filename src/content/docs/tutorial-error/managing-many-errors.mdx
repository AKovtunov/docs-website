---
title: Manage many error occurrences
metaDescription: Learn how to monitor your system so you can quickly identify and resolve many error occurrences fast. 
---

import apmErrorsInboxPage from 'images/apm_screenshot-crop_errors-inbox-page.webp'

import apmErrorsAnomaly from 'images/apm_screenshot-crop_errors-anomaly.webp'

Errors happen. Stating that is probably redundant, but working with highly complex, distributed systems inevitably leads to errors. Some of these errors might be unrelated to performance issues or outages, but generally errors will happen and they will affect customer experience. Understanding how to broach a high occurrence of errors can expedite the resolution and retrospective process, letting you improve your app and meet business goals. 

This tutorial series guides you through handling errors in your apps. As an observability platform, New Relic ingests a ton of data about your apps every day. You can define what “normal” is for your system, create alerts that notify you when things dip below (or above) normal, and, when the inevitable error happens, use all the data you’ve collected since instrumenting to grok at the problem. 

## Objectives [#objective]

This tutorial covers these concepts:

* Looking at time series to determine whether an error is expected or unexpected
* How to collect data points to begin the triage process

## Find the anomaly 

When an outage happens, your customers are likely to experience two things. The first scenario is a lack of availability, meaning they’re unable to interact with your site’s different capabilities, like logging in, searching, or inventory browsing. In the second scenario, maybe they are able to log in or browse, but the site might be slow. Whatever the situation, something’s changed and you need to figure out what.

As we established, errors happen – sometimes an error is expected and has no effect on your site, but when an outage occurs, it’s the unexpected error you want to hunt for. When you’re looking for a change, what you’re really looking for is an anomaly in established behavior.

Let’s say you’ve been notified – either by an alert or a customer – that your site isn’t performing well. When you check New Relic, you notice a few services are alerting, but there’s enough ambiguity that you can’t stake a claim that one is the source. You choose one that connects your app to external services. We’ll call it `api-gateway`. 

Let’s take a look at this screenshot from the Errors view in the api-gateway service. To get here, you’d go to one.newrelic.com > APM > Choose the alerting service, in this case `api-gateway` > Click **Errors**. 

<img
title="Overview errors affecting your services"
alt="A screenshot showing an app with many errors"
src={apmErrorsInboxPage}
/>


This screenshot shows nine error groups with anywhere from a dozen to thousands of occurrences in your app. The important bit, though, is the time series on the far right of the waterfall view. 

Your first instinct might be that the `ActivemModel:::ValidationError` error with 4k occurrences is where you should start, but if you look at the time series, its peaks and troughs are relatively consistent. Combined with the high error occurrence, you can infer that this error is expected, existed outside the recent change, and can safely be ignored or triaged later. We can eliminate number 1. 
The `Net::OpenTimeOut` has a similar pattern. Consistent peaks and troughs that extend before the incident. In this case, error groups 3, 4, and 5 have the same group name and similar patterns. We can eliminate those, too.


That leaves the fifth option, `JsonapiClient:::Notfound`. It doesn’t have many occurrences, but the timeseries is anomalous enough that it’s worth digging a bit deeper. The next screenshot extends the timeseries to 12 hours ago:

<img
title="Overview errors affecting your services"
alt="A screenshot showing an app with many errors"
src={apmErrorsAnomaly}
/>

Each timeseries and graph adjusts when you change the time parameter. With this vantage point, you can see anomalous behavior in  `JsonapiClient:::Notfound` reaches its peak. 

Knowing when something happened is a critical piece of information for answering what changed? With this, it’s time to collect your data points. 

## Collect your data points [#data]
